{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detection of sarcasm in sentences - \n",
    "In this notebook, we have tried 3 methods for detecting sarcasm in sentences - \n",
    "1. Basic Linear Regression\n",
    "2. Bidirectional LSTM\n",
    "3. Attention-Like Classifier\n",
    "\n",
    "We have tried these methods on \"News Headlines Dataset For Sarcasm Detection\" in Kaggle. We have reached 87% accuracy in the third method.\n",
    "\n",
    "Embedding 'fasttext-wiki-news-subwords-300' currently gives bad results. Check why.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports ##\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import re\n",
    "import multiprocessing as mp\n",
    "num_procs =  mp.cpu_count()\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.nn import functional as F\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 199.5/199.5MB downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asaf Karnieli\\Anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "model_type = \"glove-twitter-50\" #'fasttext-wiki-news-subwords-300' # \n",
    "word_embedding_model = api.load(model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>28619.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.476397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.499451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       is_sarcastic\n",
       "count  28619.000000\n",
       "mean       0.476397\n",
       "std        0.499451\n",
       "min        0.000000\n",
       "25%        0.000000\n",
       "50%        0.000000\n",
       "75%        1.000000\n",
       "max        1.000000"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_url = \"https://raw.githubusercontent.com/asafkar/sarcarsm_detection/master/Sarcasm_Headlines_Dataset_v2.json\"\n",
    "\n",
    "data_url = os.path.join(os.getcwd(),\"Sarcasm_Headlines_Dataset_v2.json\")\n",
    "\n",
    "# Read the Data, remove \"article_link\", and display properties\n",
    "df = pd.read_json(open(data_url, \"r\", encoding=\"utf8\"), lines=True)\n",
    "df = df[['headline','is_sarcastic']]\n",
    "df['headline'] = df['headline'].apply(lambda x: x.lower())\n",
    "df['headline'] = df['headline'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go over the data. \n",
    "# data_padded is the data padded, in order to receive fixed length sentences\n",
    "\n",
    "def word2vec(word):\n",
    "    try:\n",
    "        return word_embedding_model[word]\n",
    "    except:\n",
    "        return word_embedding_model['unk']\n",
    "\n",
    "def sentence2vecs(sentence):\n",
    "    return [word2vec(x) for x in sentence.split()]\n",
    "    \n",
    "    \n",
    "def second_longest_sentence():\n",
    "    longest_sentence = 0\n",
    "    second_longest = 0\n",
    "    for sents in df['headline'].values:\n",
    "        if len(sents.split()) > longest_sentence:\n",
    "            second_longest = longest_sentence\n",
    "            longest_sentence = len(sents.split())\n",
    "    return second_longest\n",
    "\n",
    "\n",
    "# word2vec - For each sample (sentence), turn each word into a word-embedding\n",
    "\n",
    "\n",
    "corpus_size = df['headline'].size\n",
    "word2vec_model_size = len(word_embedding_model['unk'])\n",
    "model_sentence_length = second_longest_sentence()\n",
    "data_padded = np.zeros((corpus_size, model_sentence_length , word2vec_model_size))\n",
    "data = []\n",
    "labels = np.zeros(corpus_size, dtype=np.int)\n",
    "\n",
    "for ii in range(df.shape[0]):\n",
    "    sents = df['headline'].iloc[ii]\n",
    "    labels[ii]= df['is_sarcastic'].iloc[ii]\n",
    "    \n",
    "    vectors = sentence2vecs(sents)  # size = word2vec_model_size*sentence_length\n",
    "    data.append(vectors)\n",
    "    for jj, vector in enumerate(vectors):\n",
    "        if jj == model_sentence_length:\n",
    "            break\n",
    "        data_padded[ii, jj,:] = np.asarray(vector)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split to Train and Test (For both padded and unpadded)\n",
    "training_samples  = corpus_size*9//10\n",
    "validation_samples = corpus_size - training_samples\n",
    "\n",
    "\n",
    "indices = np.arange(corpus_size)\n",
    "np.random.shuffle(indices)\n",
    "data_padded = data_padded[indices]\n",
    "labels = labels[indices]\n",
    "X_train = data_padded[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "X_val = data_padded[training_samples: training_samples + validation_samples]\n",
    "y_val = labels[training_samples: training_samples + validation_samples]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*First Attempt - Linear Regression*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asaf Karnieli\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic regression classifier on training set: 0.80\n",
      "Accuracy of Logistic regression classifier  on test set: 0.78\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "X_train_flat = X_train.reshape(-1, model_sentence_length*word2vec_model_size)\n",
    "X_val_flat = X_val.reshape(-1, model_sentence_length*word2vec_model_size)\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train_flat, y_train)\n",
    "print('Accuracy of Logistic regression classifier on training set: {:.2f}'\n",
    "     .format(log_reg.score(X_train_flat, y_train)))\n",
    "print('Accuracy of Logistic regression classifier  on test set: {:.2f}'\n",
    "     .format(log_reg.score(X_val_flat, y_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Second Attempt - Bidirectional LSTM*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU or GPU :\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters\n",
    "sequence_length = model_sentence_length\n",
    "input_size =      word2vec_model_size\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_classes = 2\n",
    "batch_size = 64\n",
    "num_epochs = 5\n",
    "learning_rate = 0.005\n",
    "lstm_dropout = 0.1\n",
    "\n",
    "\n",
    "# Data loader\n",
    "train_dataset = torch.utils.data.TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).long())\n",
    "train_loader =  torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataset = torch.utils.data.TensorDataset(torch.from_numpy(X_val).float(), torch.from_numpy(y_val).long())\n",
    "test_loader =  torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "              \n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Bidirectional LSTM (many to one)\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, lstm_dropout):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size*2, num_classes)  # 2 for bidirection\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Set initial states\n",
    "        h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device) # 2 for bidirection \n",
    "        c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size*2)\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "model = BiLSTM(input_size, hidden_size, num_layers, num_classes, lstm_dropout).to(device)\n",
    "\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/403], Loss: 0.7089\n",
      "Epoch [1/5], Step [200/403], Loss: 0.6193\n",
      "Epoch [1/5], Step [300/403], Loss: 0.5530\n",
      "Epoch [1/5], Step [400/403], Loss: 0.4670\n",
      "Epoch [2/5], Step [100/403], Loss: 0.5968\n",
      "Epoch [2/5], Step [200/403], Loss: 0.4371\n",
      "Epoch [2/5], Step [300/403], Loss: 0.4401\n",
      "Epoch [2/5], Step [400/403], Loss: 0.2820\n",
      "Epoch [3/5], Step [100/403], Loss: 0.4800\n",
      "Epoch [3/5], Step [200/403], Loss: 0.4245\n",
      "Epoch [3/5], Step [300/403], Loss: 0.3525\n",
      "Epoch [3/5], Step [400/403], Loss: 0.2356\n",
      "Epoch [4/5], Step [100/403], Loss: 0.4033\n",
      "Epoch [4/5], Step [200/403], Loss: 0.3918\n",
      "Epoch [4/5], Step [300/403], Loss: 0.2500\n",
      "Epoch [4/5], Step [400/403], Loss: 0.1784\n",
      "Epoch [5/5], Step [100/403], Loss: 0.3350\n",
      "Epoch [5/5], Step [200/403], Loss: 0.2731\n",
      "Epoch [5/5], Step [300/403], Loss: 0.1698\n",
      "Epoch [5/5], Step [400/403], Loss: 0.0867\n",
      "Train Accuracy of the model on the 10000 test sents: 93.47361882206779 %\n",
      "Test Accuracy of the model on the 10000 test sents: 86.16352201257861 %\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (sents, labels) in enumerate(train_loader):\n",
    "#         print(sents.shape)\n",
    "        sents = sents.reshape(-1, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(sents)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "# Test the train accuracy\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for sents, labels in train_loader:\n",
    "        sents = sents.reshape(-1, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(sents)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Train Accuracy of the model on the 10000 test sents: {} %'.format(100 * correct / total)) \n",
    "            \n",
    "\n",
    "# Test the model\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for sents, labels in test_loader:\n",
    "        sents = sents.reshape(-1, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(sents)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test sents: {} %'.format(100 * correct / total)) \n",
    "\n",
    "# Save the model checkpoint\n",
    "# torch.save(model.state_dict(), 'model.ckpt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Third Attempt - Attention*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU or GPU :\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters\n",
    "sequence_length = model_sentence_length\n",
    "input_size =      word2vec_model_size  # embedding size\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_classes = 2\n",
    "batch_size = 64\n",
    "num_epochs = 8\n",
    "learning_rate = 0.005\n",
    "lstm_dropout = 0.1  # dropout between 2 lstm layers\n",
    "alpha_dropout = 0.3 # dropout between attention output and fc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas are from the following paper - *Attention-Based Bidirectional Long Short-Term Memory Networks for\n",
    "Relation Classification*\n",
    "https://www.aclweb.org/anthology/P16-2034"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention Model \n",
    "class AttentionModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, lstm_dropout, sequence_length, alpha_dropout):\n",
    "        super(AttentionModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True, dropout=lstm_dropout)\n",
    "        \n",
    "        self.alpha_net = nn.Linear(hidden_size*2*sequence_length, sequence_length)\n",
    "        self.dropout = nn.Dropout(alpha_dropout)\n",
    "        self.fc = nn.Linear(hidden_size*2, num_classes)  # 2 for bidirection\n",
    "        \n",
    "    def attention_layer(self, input_from_lstm):\n",
    "        # H = {h1,h2...,ht} = input_from_lstm\n",
    "        # alpha = softmax(w*M)\n",
    "        M = nn.Tanh()(input_from_lstm).permute(1,0,2) # torch.Size([64, 29, 256]) - batch, sequence_length, hidden*2\n",
    "        \n",
    "        wM_flat = self.alpha_net(M.reshape(-1, hidden_size*2*sequence_length))\n",
    "        wM = wM_flat.reshape(-1,sequence_length)\n",
    "        alpha_weights = F.softmax(wM, 1).unsqueeze(2) # includes w (weights) - sized hidden*2 - unsqueeze to add axis for bmm\n",
    "        \n",
    "        # r = H*alpha\n",
    "        #  bmm shape should be (b×n×m) and (b×m×p) then only it will give (b×n×p) as the output shape\n",
    "        r = torch.bmm(input_from_lstm.permute(1,2,0), alpha_weights).squeeze()\n",
    "        return r        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Set initial states\n",
    "        h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device) # 2 for bidirection \n",
    "        c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        # lstm_out: tensor of shape (batch_size, seq_length, hidden_size*2) - \n",
    "        # tensor containing the output features (h_t) from the last layer of the LSTM, for each t\n",
    "        \n",
    "        # last_hidden_state : (num_layers*2, batch_size, hidden_size) -\n",
    "        #  tensor containing the hidden state for t = seq_len ; used for backprop...\n",
    "        lstm_out, (last_hidden_state, last_cell_state) = self.lstm(x, (h0, c0)) \n",
    "        \n",
    "        attention_out = self.attention_layer(lstm_out.permute(1,0,2))\n",
    "        h_star = nn.Tanh()(attention_out)\n",
    "        out = self.fc(self.dropout((h_star)))\n",
    "        return out\n",
    "\n",
    "model = AttentionModel(input_size, hidden_size, num_layers, num_classes, lstm_dropout, \n",
    "                       sequence_length, alpha_dropout).to(device)\n",
    "\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.0001)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/8], Step [100/403], Loss: 0.7031\n",
      "Epoch [1/8], Step [200/403], Loss: 0.4718\n",
      "Epoch [1/8], Step [300/403], Loss: 0.4175\n",
      "Epoch [1/8], Step [400/403], Loss: 0.3866\n",
      "Epoch [2/8], Step [100/403], Loss: 0.4835\n",
      "Epoch [2/8], Step [200/403], Loss: 0.4381\n",
      "Epoch [2/8], Step [300/403], Loss: 0.3637\n",
      "Epoch [2/8], Step [400/403], Loss: 0.3159\n",
      "Epoch [3/8], Step [100/403], Loss: 0.3989\n",
      "Epoch [3/8], Step [200/403], Loss: 0.3980\n",
      "Epoch [3/8], Step [300/403], Loss: 0.2889\n",
      "Epoch [3/8], Step [400/403], Loss: 0.2881\n",
      "Epoch [4/8], Step [100/403], Loss: 0.3634\n",
      "Epoch [4/8], Step [200/403], Loss: 0.2546\n",
      "Epoch [4/8], Step [300/403], Loss: 0.1972\n",
      "Epoch [4/8], Step [400/403], Loss: 0.2184\n",
      "Epoch [5/8], Step [100/403], Loss: 0.3648\n",
      "Epoch [5/8], Step [200/403], Loss: 0.2365\n",
      "Epoch [5/8], Step [300/403], Loss: 0.1854\n",
      "Epoch [5/8], Step [400/403], Loss: 0.2148\n",
      "Epoch [6/8], Step [100/403], Loss: 0.3447\n",
      "Epoch [6/8], Step [200/403], Loss: 0.2361\n",
      "Epoch [6/8], Step [300/403], Loss: 0.1755\n",
      "Epoch [6/8], Step [400/403], Loss: 0.1893\n",
      "Epoch [7/8], Step [100/403], Loss: 0.3253\n",
      "Epoch [7/8], Step [200/403], Loss: 0.2296\n",
      "Epoch [7/8], Step [300/403], Loss: 0.1755\n",
      "Epoch [7/8], Step [400/403], Loss: 0.2152\n",
      "Epoch [8/8], Step [100/403], Loss: 0.3094\n",
      "Epoch [8/8], Step [200/403], Loss: 0.2680\n",
      "Epoch [8/8], Step [300/403], Loss: 0.1746\n",
      "Epoch [8/8], Step [400/403], Loss: 0.2049\n",
      "Train Accuracy of the model on the 10000 test sents: 92.31665178398106 %\n",
      "Test Accuracy of the model on the 10000 test sents: 87.03703703703704 %\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "scheduler = StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (sents, labels) in enumerate(train_loader):\n",
    "        sents = sents.reshape(-1, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(sents)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "#         scheduler.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "            \n",
    "    scheduler.step()\n",
    "\n",
    "# Test the train accuracy\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for sents, labels in train_loader:\n",
    "        sents = sents.reshape(-1, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(sents)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Train Accuracy of the model on the 10000 test sents: {} %'.format(100 * correct / total)) \n",
    "            \n",
    "\n",
    "# Test the model\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for sents, labels in test_loader:\n",
    "        sents = sents.reshape(-1, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(sents)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test sents: {} %'.format(100 * correct / total)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
